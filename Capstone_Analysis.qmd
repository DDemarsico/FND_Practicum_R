---
title: "Capstone Project"
format: html
editor: visual
---

# Capstone Project:

This will be the analysis for my final capstone project. This project's focus is to both understand why certain individuals are not likely to stay in a group therapy setting at the FND clinic. To do this, we have two sets of analyses:

1.  An interpretable logistic regression model where we can quantify the effect of our covariates on the outcome.

2.  A machine learning logistic regression model, where we can accurately predict which patients are at-risk for non-adherence to the treatment.

The dataset I'll be working with is the cleaned dataset from my Practicum. This should contain almost all of the data I'll need.

# Packages

```{r}
library(tidyverse)  # For tidyverse programming
library(ggplot2)    # For Data Visualizations
library(plotly)     # For interactive visualizations
library(gt)         # For nice table 1s
library(gtsummary)  # Used for pretty table 1s
library(caret)      # Used for Yeo-Johnson transformation
library(pROC)       # Used for ROC curves
library(glmnet)    # For Lasso regression
```

# Data Import

Great, let's import the new dataset to get everything started

```{r}

data <- read.csv("C:\\Users\\domin\\Downloads\\Practicum_Cleaned_Data.csv")
```

# Data Overview:

Ok, now that we have our dataset, let's get some descriptive statistics out. For both of our aims, our primary outcome measure will be whether or not the subject completed greater than 50% of their group visits. This is measured by the variable group_complete_50. Let's take a peak at the distribution of this variable

```{r}

table(data$group_complete_50)

# 258 patients completed less than 50% of their visits, and 359 subjects completed greater than 50% of their visits. This leads to a total of 617 patients used in our study.

# Now let's s

```

Great, 617 patients will be used in our final analysis. Let's create a new dataframe that reflects these patients.

```{r}

# First, let's set a new dataframe called data2. This will be populated with the 617 subjects who had attendance data. 

data2 <- data[!is.na(data$group_complete_pct), ]

summary (data2$group_complete_50)

summary(data2)
```

## Table 1

Great, now let's create a nice Table 1 to see the patient characteristics of our population.

```{r}
tbl1 <- data2 %>%
  select(group_complete_50, tblPatient..c_CurrentAge, tblPatient..Gender, tblPatient..Gender, race, insurance, Sx_Dx_time, trauma_events, baseline_seizure, tblPatient..c_lastIEmployment, depression_any, anxiety_any, tblPatient..maritalStatus, disability) %>%
  tbl_summary(by = group_complete_50) %>%
  modify_header(label ~ "**Variable**") %>%
  bold_labels() %>%
  modify_caption("**Table 1.** Baseline Characteristics") %>%
  as_gt()

tbl1
```

### Summary

Ok, let's summarize some results: Individuals who complete less than 50% tend to

-   Be a bit younger (36 years vs 39)

-   Be female (81% vs 72%)

-   Be non-white (28% vs 20%)

-   Be on medicaid (45% vs 35%)

-   Not be on Private insurance (36% vs 47%)

-   Unemployed (61% vs 49%)

-   Tend to not have a partner (mixed)

Most of these results are expected. Younger, non-white, medicaid patients would be the quintessential example of an individual who would be affected by socioeconomic factors. However, unemployed individuals not completing the program have more observations than employed individuals.

Additionally, individuals who report not having a SO seem to also have a greater presence in the non adherent group.

However, medical issues, such as depresion, anxiety, baseline seizure amount, and amount of traumatic events seem to have no difference within their population.

# Data Normalization

Now that we know the general distribution of our data, let's begin going through our covariates to assure the data is normal.

## Age:

Let's take a look at the age distribution:

```{r}

# Let's see our mean values
summary(data2$tblPatient..c_CurrentAge)

# Mean is 39.88 and Median is 37.7. The youngest is 19 and the oldest is 86.9

# Let's start with a histogram to assess normality

hist(data2$tblPatient..c_CurrentAge)

# Relatively right-skewed. 
```

Ok, since this is slightly skewed, let's llog transform this variable.

```{r}

data2$age_log <- log(data2$tblPatient..c_CurrentAge)

# Now let's check the results:

hist(data2$age_log)
```

Great, this is relatively normal.

## Gender:

Ok, gender as a variable hasn't been cleaned yet. Let's start by changing the character variable into an categorical variable:

```{r}

data2$gender <- factor(data2$tblPatient..Gender)

summary(data2$gender)


```

## Race:

Race is still a character, let's change it to a factor. Before making it a character, non-white patients were grouped together into a "Other" category. This is utterly reductive, but the clinic would not have a large enough cohort to study non-white patients without this.

```{r}

data2$race <- factor(data2$race)

summary(data2$race)
```

A bit skewed towards white patients, but nothing else to be done.

## Insurance:

Again, insurance is already cleaned, but needs to be transformed into a factor.

```{r}

data2$insurance <- factor(data2$insurance)

plot1 <- ggplot(data2, aes(x = fct_rev(fct_infreq(insurance)), fill = insurance)) +
            geom_bar() +
            scale_fill_brewer(palette = "Set3") +
            theme_minimal() +
            labs(title = "Insurance Distributions", x = "Insurance Plans", y = "Number of Patients") + 
            theme(axis.text.x = element_text(angle = 90, hjust = 1))  
ggplotly(plot1)   # Convert to interactive plotly chart
```

## Sx_Dx_Time

This variable is measured as the amount of months between symptom onset and the patient receiving a diagnosis. Let's take a look at the variable:

```{r}

summary(data2$Sx_Dx_time)

# 82.14 months on average. Let's check a plot for rough normality

hist(data2$Sx_Dx_time, breaks = 30, main = "Histogram", xlab = "Sx_Dx_time")

# Wow, incredibly right skewed. Let's do another log transform
data2$Sx_Dx_Log <- log(data2$Sx_Dx_time)

# Now let's look again
hist(data2$Sx_Dx_Log, breaks = 50, main = "Histogram", xlab = "Sx_Dx_Log")


```

Still not working. Since we have patients that have 0 months, we'll need to try a Yeo Johnson Transformation to achieve normality.

```{r}

# Pre process the data using the YeoJohnson transformation
preproc <- preProcess(data2["Sx_Dx_time"], method = "YeoJohnson")
# Then save the predicted values into another value
data2$SxDx_yj <- predict(preproc, data2["Sx_Dx_time"])[[1]]

# Ok, now let's look at the data
hist(data2$SxDx_yj, 
     main = "Histogram of Yeo-Johnson Transformed Variable", 
     xlab = "Transformed Values", 
     col = "lightblue", 
     breaks = 30)
```

## Traumatic Events:

Event \# is a continuous variable, so let's check the values:

```{r}

hist(data2$trauma_events)

# a bit right skewed, but not horrible.
```

## Employment:

This is also a character that needs to be changed to a factor:

```{r}
# Set this as a factor
data2$employment <- factor(data2$tblPatient..c_lastIEmployment)
# Now let's review
summary(data2$employment)

# Now let's make this a new factor that has two levels: Job or No-Job

# Create new binary factor based on first level
data2$employment <- ifelse(data2$employment == levels(data2$employment)[1],
                                  "Employed", "Unemployed")

# Convert to factor
data2$employment <- factor(data2$employment, levels = c("Employed", "Unemployed"))

# And review it:
summary(data2$employment)
```

## Disability

This is also a character, but it needs to be a factor:

```{r}

# Set this as a factor
data2$disability <- factor(data2$disability, levels = c("No Disability", "Disability"))
                           
# Now let's review
table(data2$disability)
table(data$disability)

```

## Relationship Status:

Our current variable for relationship status is a string with multiple values. I want to set this to be a factor that has 3 levels. For my purposes, I want it to be Partner, No Partner, or Unknown.

```{r}

# Let's start by looking at our original variable:
table(data2$tblPatient..maritalStatus)

# For the sake of our new factor I want Engaged, Married, Significant Other to be put into Partner. I want Other and Unknown to be in Unknown, and everything else to be in No Partner.

data2 <- data2 %>%
  mutate(
    partner = case_when(
      tblPatient..maritalStatus %in% c("Engaged", "Married", "Significant Other") ~ "Partner",
      tblPatient..maritalStatus == "Unknown" ~ "Unknown",
      tblPatient..maritalStatus == "Other" ~ "Unknown",
      !is.na(tblPatient..maritalStatus) ~ "No Partner",
      TRUE ~ NA_character_
    ),
    partner = factor(partner, levels = c("Partner", "No Partner", "Unknown"))
  )

# Let's review the the factor to check our values:
table(data2$partner)
```

# Exploratory Variables

I also want to clean some other variables. These may not make too much sense in a model focused on Parsimony. However, they can be useful in a machine learning context where we can fit up to 50-60 variables at max. However, a model with 10 or less would probably be best. This dataset isn't big

# Aim 1:

## Overview:

The primary goal of Aim 1 is to fit a logistic regression model to better understand which covariates contribute to any specific patient's risk for non-adherence to the group therapy modality.

## Saturated Model:

To begin, I want to start with a saturated model, but only using the variable that

# Aim 2:

## Dataframe creation

Let's try to train the logistic regression model first.

Before running, let's look for NAs. This will cause problems for us in the model training section.

```{r}

# Let's do one check of our class distribution again:
summary(data2$group_complete_50)
prop.table(table(data2$group_complete_50))
# Good, 258 High risk, and 359 low risk. 41.8% to 58.2%. Not too different.No need to rebalance the classes.

#set Group_Complete_50 as a factor:
data2$group_complete_50 <- factor(data2$group_complete_50)

# And set the names to be better suited to the model:
levels(data2$group_complete_50) <- c("HighRisk", "LowRisk") 


# Ok we have a few missing points, let's create a model training dataset to see if we can run the model first.

data2_model <- na.omit(data2[, c("group_complete_50", "age_log", "insurance", "gender", "race", "employment", "baseline_seizure", "anxiety_any", "depression_any", "disability", "partner", "Sx_Dx_time", "tblPatient..Ethnicity")])

# And now let's set our reference to Low-Risk, as we're only interested in identifying patients that are high risk.
data2_model$group_complete_50 <- relevel(data2_model$group_complete_50, ref = "LowRisk")
```

## Model training

```{r}
# Define the control for cross-validation
train_control <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary, savePredictions = "final")


# Train the logistic regression model
model_simple1 <- train(
  group_complete_50 ~ age_log + insurance + gender + race + employment + baseline_seizure + anxiety_any + depression_any + disability + partner + Sx_Dx_time, 
  data = data2_model,
  method = "glm",
  family = "binomial",
  trControl = train_control
)

# View results
print(model_simple1)
summary(model_simple1$finalModel)

#the data isn't significant, but let's exponentiate the coefficients to get ORs
exp(coef(model_simple1$finalModel))
```

The total accuracy may be a bit low, but the sensitivity is quite high. This actually may be quite helpful for the sake of the clinic. In this case, High-Risk patients are considered the positive cases. This model was able to correctly identify High-Risk patients approximately \~80% of the time. However, it was only able to correctly indentify low-risk patients 23% of the time.

In an epidemiological sense, this effectively states that we have a quite low rate of false negatives, but quite a lot of false positives. For the purpose of Aim 2, this is actually quite good. The model may inflate the number of patients by putting truly low-risk patients in the high-risk group, but very rarely misses people who truly are High-risk. For the purpose of providing additional resources, this is highly beneficial. Patients will not be turned away due to this model.

```{r}

# Extract predicted probabilities and observed outcomes
pred <- model_simple1$pred

# Check the levels to ensure correct positive class
levels(pred$obs)  # positive class = second level

# Create ROC curve object
roc_obj <- roc(response = pred$obs, predictor = pred$HighRisk)

# Plot ROC
plot(roc_obj, col = "blue", main = "ROC Curve")

```

## Recursive Feature Elimination:

Now that we have a model that is *AT LEAST* sensitive, let's try to make this model more accurate overall. To do this, I plan to retrain the model with an iteratively fewer number of covariates. This way, we aim to reduce the overfitting and signal to noise ratio.

```{r}

# Define predictor names
features <- c("age_log", "insurance", "gender", "race", "employment", "baseline_seizure", "anxiety_any", "depression_any", "disability", "partner", "Sx_Dx_time")

# Define control parameters for RFE
rfe_control <- rfeControl(functions = rfFuncs, # Using Random Forest for feature importance
                         method = "cv",        # Cross-validation
                         number = 10)           # Number of folds

# Run RFE
rfe_results <- rfe(x = data2_model[, features],  # Predictor variables
                  y = data2_model[, "group_complete_50"],    # Outcome variable
                  sizes = c(1:11),       # Number of features to try
                  rfeControl = rfe_control)

# Get the selected features
selected_features <- predictors(rfe_results)

# Print results
print(rfe_results)
print(selected_features)

```

When viewing the effects of the model, the top 5 most important variables are Sx_Dx_time, baseline seizure count, employment, insurance, and disability.

However, the model with the best accuracy (.5974), was a model with 10 variables: Sx_Dx_time, baseline seizure, employment, insurance, disability, race, gender, anxiety_any, partner, age_log.

# Lasso 

Now that we have our final variables, I want to compare the results with Lasso regression. Let's move onto seeing which are actually needed and which are redundant. Lasso will move certain covariates to 0 if others are more predictive.

```{r}


# Select the 10 RFE-selected variables
rfe_vars <- c("age_log", "insurance", "gender", "race", "employment", "baseline_seizure", "anxiety_any", "disability", "partner", "Sx_Dx_time") 

# 2. Subset the data — ensure all selected variables exist and have no missing
# Subset and remove rows with missing values
selected_data <- na.omit(data2_model[, c("group_complete_50", rfe_vars)])

# Convert factor/character to numeric 0/1
selected_data$group_complete_50 <- ifelse(selected_data$group_complete_50 == "LowRisk", 1, 0)


# 3. Remove missing rows (if any)
selected_data <- na.omit(selected_data)


# 4. Create model matrix (excluding intercept)
x <- model.matrix(group_complete_50 ~ ., data = selected_data)[, -1]
y <- selected_data$group_complete_50



```

Now we can run cross validated Lasso:

```{r}

set.seed(123)

cv_lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial", type.measure = "class")  # for accuracy
# or use type.measure = "auc" for AUC optimization

# Predict probabilities
pred_probs <- predict(cv_lasso, newx = x, s = "lambda.min", type = "response")

# Predict classes
pred_class <- ifelse(pred_probs > 0.5, 1, 0)

# Accuracy
mean(pred_class == y)

# AUC
roc_obj <- roc(y, as.numeric(pred_probs))  # Ensure pred_probs is numeric
auc(roc_obj)

# Plot ROC
plot(roc_obj, col = "blue", main = "ROC Curve")
```

Pretty solid, but now i want to run this model using Random Forest first.

# Random Forest:

I ran this model first with logistic regression, but this was hitting and accuracy limit of approximately 60%. This however, did have a high sensitivity, but was definitely lacking in terms of overall prediction. I'd like to see if Random Forest can provide a higher predictive value if we sacrifice interpretability. We'll start with rerunning RFE for Random Forest.

```{r}

# First we'll set our variable names to be used.
predictors <- c("age_log", "insurance", "gender", "race", "employment", "baseline_seizure", "anxiety_any", "depression_any", "disability", "partner", "Sx_Dx_time", "tblPatient..Ethnicity")


set.seed(123)



ctrl <- rfeControl(functions = rfFuncs,
                   method = "cv",
                   number = 10)

rfe_rf <- rfe(x = data2_model[, predictors],   # predictors is vector of var names
              y = data2_model$group_complete_50,
              sizes = c(5, 10, 15),   # try subsets of variables
              rfeControl = ctrl)


# Shows best variable set for random forest
rfe_rf$optVariables 
```

RFE tailored for Random Forest suggests only 5 variables: Baseline_seizure, Sx_Dx_time, employment, insurance, and Ethnicity.

Great, now using these 5 variables, let's run the random forest model.

```{r}

# Set the selected variables in their own list
selected_vars <- c("baseline_seizure", "Sx_Dx_time", "employment", "insurance", "tblPatient..Ethnicity")

# create a new dataset with the cleaned values
rf_data <- data2_model[, c("group_complete_50", selected_vars)]

# Remove rows with missing values if they are present
rf_data <- na.omit(rf_data)

# And set LowRisk as the reference:
rf_data$group_complete_50 <- relevel(rf_data$group_complete_50, ref = "LowRisk")

# And finally, check the distribution of values in the factor:
table(rf_data$group_complete_50)
```

Now run the model:

```{r}

# Use 10-fold CV and evaluate on AUC
ctrl <- trainControl(method = "cv",
                     number = 5,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)

rf_model <- train(group_complete_50 ~ .,
                  data = rf_data,
                  method = "rf",
                  trControl = ctrl,
                  metric = "ROC",      # Optimize for AUC
                  tuneLength = 5)      # Auto-tunes mtry

print(rf_model)
```

Excellent, the random forest was

## Plotting:

First, let's plot the best threshold when comparing sensitivity and specificity.

```{r}

# First 
roc_obj <- roc(rf_data$group_complete_50, pred_probs)

plot(roc_obj, print.thres = "best", print.thres.best.method = "closest.topleft")
```

Ok, but for our purposes, sensitivity to High-risk patients is more important. Let's look at some thresholds for this model instead.

```{r}

# Your true outcomes and predicted probabilities
y_true <- rf_data$group_complete_50
pred_probs <- predict(rf_model, newdata = rf_data, type = "prob")[, "HighRisk"]

# Create ROC object
roc_obj <- roc(y_true, pred_probs)

# Extract thresholds, sensitivities, and specificities
thresholds <- roc_obj$thresholds
sensitivities <- roc_obj$sensitivities
specificities <- roc_obj$specificities

# Plot sensitivity and specificity vs thresholds
plot(thresholds, sensitivities, type = "l", col = "blue", ylim = c(0,1),
     xlab = "Threshold", ylab = "Rate", main = "Sensitivity and Specificity vs Threshold")
lines(thresholds, specificities, col = "red")
legend("right", legend = c("Sensitivity", "Specificity"), col = c("blue", "red"), lty = 1)

# Choose the threshold. .3 seems appropriate.
chosen_threshold <- 0.3

# Add vertical line
abline(v = chosen_threshold, col = "darkgreen", lwd = 2, lty = 2)

# Find nearest index to chosen_threshold
idx <- which.min(abs(thresholds - chosen_threshold))

# Add text labels for sensitivity and specificity at that threshold
text(x = chosen_threshold, y = sensitivities[idx] + 0.05, 
     labels = paste0("Sens: ", round(sensitivities[idx], 2)), col = "blue", pos = 4)
text(x = chosen_threshold, y = specificities[idx] - 0.05, 
     labels = paste0("Spec: ", round(specificities[idx], 2)), col = "red", pos = 4)
```

Ok, great! When looking at a threshold of 0.3, We receive a sensitivity of .85 and a specificity of 0.61. This values went up as our threshold for determining who was at-risk changed. Looking at this graph, if we moved the dotted line to .5, our sensitivity goes up, but our specificity tanks. By being more liberal with who is considered High-Risk, we can increase the odds of allowing certain patients receive additional resources, while simultaneously factoring out the patients that are truly low risk. By lowering the bar, we choose to include edge cases, while having a good shot of making sure that the lowest risk patients aren't entered in, saving resources.
