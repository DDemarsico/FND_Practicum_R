---
title: "Capstone Project"
format: html
editor: visual
---

# Capstone Project:

This will be the analysis for my final capstone project. This project's focus is to both understand why certain individuals are not likely to stay in a group therapy setting at the FND clinic. To do this, we have two sets of analyses:

1.  An interpretable logistic regression model where we can quantify the effect of our covariates on the outcome.

2.  A machine learning logistic regression model, where we can accurately predict which patients are at-risk for non-adherence to the treatment.

The dataset I'll be working with is the cleaned dataset from my Practicum. This should contain almost all of the data I'll need.

# Packages

```{r}
library(tidyverse)  # For tidyverse programming
library(ggplot2)    # For Data Visualizations
library(plotly)     # For interactive visualizations
library(gt)         # For nice table 1s
library(gtsummary)  # Used for pretty table 1s
library(caret)      # Used for Yeo-Johnson transformation
library(pROC)       # Used for ROC curves
library(glmnet)     # For Lasso regression
library(pwr)        # Used for Power Analysis
library(powerMediation) # Used for Power Analysis
library(car)            # Used for VIF  
```

# Data Import

Great, let's import the new dataset to get everything started

```{r}

data <- read.csv("C:\\Users\\domin\\Downloads\\Practicum_Cleaned_Data.csv")
```

# Data Overview:

Ok, now that we have our dataset, let's get some descriptive statistics out. For both of our aims, our primary outcome measure will be whether or not the subject completed greater than 50% of their group visits. This is measured by the variable group_complete_50. Let's take a peak at the distribution of this variable

```{r}

table(data$group_complete_50)

# 258 patients completed less than 50% of their visits, and 359 subjects completed greater than 50% of their visits. This leads to a total of 617 patients used in our study.

# Now let's s
print(258/617)

```

Great, 617 patients will be used in our final analysis. Let's create a new dataframe that reflects these patients.

```{r}

# First, let's set a new dataframe called data2. This will be populated with the 617 subjects who had attendance data. 

data2 <- data[!is.na(data$group_complete_pct), ]

summary (data2$group_complete_50)

summary(data2)
```

## Table 1

Great, now let's create a nice Table 1 to see the patient characteristics of our population.

```{r}
tbl1 <- data2 %>%
  select(group_complete_50, tblPatient..c_CurrentAge, tblPatient..Gender, tblPatient..Gender, race, insurance, Sx_Dx_time, trauma_events, baseline_seizure, tblPatient..c_lastIEmployment, depression_any, anxiety_any, tblPatient..maritalStatus, disability) %>%
  tbl_summary(by = group_complete_50) %>%
  modify_header(label ~ "**Variable**") %>%
  bold_labels() %>%
  modify_caption("**Table 1.** Baseline Characteristics") %>%
  as_gt()

tbl1
```

Now let's do totals:

```{r}


tbl2 <- data2 %>%
  select(group_complete_50, tblPatient..c_CurrentAge, tblPatient..Gender, tblPatient..Gender, race, insurance, Sx_Dx_time, trauma_events, baseline_seizure, tblPatient..c_lastIEmployment, depression_any, anxiety_any, tblPatient..maritalStatus, disability) %>%
  tbl_summary %>%
  modify_header(label ~ "**Variable**") %>%
  bold_labels() %>%
  modify_caption("**Table 1.** Baseline Characteristics") %>%
  as_gt()

tbl2
```

### Summary

Ok, let's summarize some results: Individuals who complete less than 50% tend to

-   Be a bit younger (36 years vs 39)

-   Be female (81% vs 72%)

-   Be non-white (28% vs 20%)

-   Be on medicaid (45% vs 35%)

-   Not be on Private insurance (36% vs 47%)

-   Unemployed (61% vs 49%)

-   Tend to not have a partner (mixed)

Most of these results are expected. Younger, non-white, medicaid patients would be the quintessential example of an individual who would be affected by socioeconomic factors. However, unemployed individuals not completing the program have more observations than employed individuals.

Additionally, individuals who report not having a SO seem to also have a greater presence in the non adherent group.

However, medical issues, such as depresion, anxiety, baseline seizure amount, and amount of traumatic events seem to have no difference within their population.

# Data Normalization

Now that we know the general distribution of our data, let's begin going through our covariates to assure the data is normal.

## Age:

Let's take a look at the age distribution:

```{r}

# Let's see our mean values
summary(data2$tblPatient..c_CurrentAge)

# Mean is 39.88 and Median is 37.7. The youngest is 19 and the oldest is 86.9

# Let's start with a histogram to assess normality

hist(data2$tblPatient..c_CurrentAge)

# Relatively right-skewed. 
```

Ok, since this is slightly skewed, let's llog transform this variable.

```{r}

data2$age_log <- log(data2$tblPatient..c_CurrentAge)

# Now let's check the results:

hist(data2$age_log)
```

Great, this is relatively normal.

## Gender:

Ok, gender as a variable hasn't been cleaned yet. Let's start by changing the character variable into an categorical variable:

```{r}

data2$gender <- factor(data2$tblPatient..Gender)

summary(data2$gender)


```

## Race:

Race is still a character, let's change it to a factor. Before making it a character, non-white patients were grouped together into a "Other" category. This is utterly reductive, but the clinic would not have a large enough cohort to study non-white patients without this.

```{r}

data2$race <- factor(data2$race)

summary(data2$race)
```

A bit skewed towards white patients, but nothing else to be done.

## Insurance:

Again, insurance is already cleaned, but needs to be transformed into a factor.

```{r}

data2$insurance <- factor(data2$insurance)

plot1 <- ggplot(data2, aes(x = fct_rev(fct_infreq(insurance)), fill = insurance)) +
            geom_bar() +
            scale_fill_brewer(palette = "Set3") +
            theme_minimal() +
            labs(title = "Insurance Distributions", x = "Insurance Plans", y = "Number of Patients") + 
            theme(axis.text.x = element_text(angle = 90, hjust = 1))  
ggplotly(plot1)   # Convert to interactive plotly chart
```

## Sx_Dx_Time

This variable is measured as the amount of months between symptom onset and the patient receiving a diagnosis. Let's take a look at the variable:

```{r}

summary(data2$Sx_Dx_time)

# 82.14 months on average. Let's check a plot for rough normality

hist(data2$Sx_Dx_time, breaks = 30, main = "Histogram", xlab = "Sx_Dx_time")

# Wow, incredibly right skewed. Let's do another log transform
data2$Sx_Dx_Log <- log(data2$Sx_Dx_time)

# Now let's look again
hist(data2$Sx_Dx_Log, breaks = 50, main = "Histogram", xlab = "Sx_Dx_Log")


```

Still not working. Since we have patients that have 0 months, we'll need to try a Yeo Johnson Transformation to achieve normality.

```{r}

# Pre process the data using the YeoJohnson transformation
preproc <- preProcess(data2["Sx_Dx_time"], method = "YeoJohnson")
# Then save the predicted values into another value
data2$SxDx_yj <- predict(preproc, data2["Sx_Dx_time"])[[1]]

# Ok, now let's look at the data
hist(data2$SxDx_yj, 
     main = "Histogram of Yeo-Johnson Transformed Variable", 
     xlab = "Transformed Values", 
     col = "lightblue", 
     breaks = 30)
```

## Traumatic Events:

Event \# is a continuous variable, so let's check the values:

```{r}

hist(data2$trauma_events)

# a bit right skewed, but not horrible.
```

## Employment:

This is also a character that needs to be changed to a factor:

```{r}
# Set this as a factor
data2$employment <- factor(data2$tblPatient..c_lastIEmployment)
# Now let's review
summary(data2$employment)

# Now let's make this a new factor that has two levels: Job or No-Job

# Create new binary factor based on first level
data2$employment <- ifelse(data2$employment == levels(data2$employment)[1],
                                  "Employed", "Unemployed")

# Convert to factor
data2$employment <- factor(data2$employment, levels = c("Employed", "Unemployed"))

# And review it:
summary(data2$employment)
```

## Disability

This is also a character, but it needs to be a factor:

```{r}

# Set this as a factor
data2$disability <- factor(data2$disability, levels = c("No Disability", "Disability"))
                           
# Now let's review
table(data2$disability)
table(data$disability)

```

## Relationship Status:

Our current variable for relationship status is a string with multiple values. I want to set this to be a factor that has 3 levels. For my purposes, I want it to be Partner, No Partner, or Unknown.

```{r}

# Let's start by looking at our original variable:
table(data2$tblPatient..maritalStatus)

# For the sake of our new factor I want Engaged, Married, Significant Other to be put into Partner. I want Other and Unknown to be in Unknown, and everything else to be in No Partner.

data2 <- data2 %>%
  mutate(
    partner = case_when(
      tblPatient..maritalStatus %in% c("Engaged", "Married", "Significant Other") ~ "Partner",
      tblPatient..maritalStatus == "Unknown" ~ "Unknown",
      tblPatient..maritalStatus == "Other" ~ "Unknown",
      !is.na(tblPatient..maritalStatus) ~ "No Partner",
      TRUE ~ NA_character_
    ),
    partner = factor(partner, levels = c("Partner", "No Partner", "Unknown"))
  )

# Let's review the the factor to check our values:
table(data2$partner)
```

## NES vs ES

And I also want to create a separate factor for patients that also have Epileptic seizures:

```{r}

# Let's check our distribution:
table(data2$tblPatient..nesES)

#463 no, 25 Unknown, 128 Yes

# Great, now let's create this to be a factor with 3 levels:
data2 <- data2 %>%
  mutate(
    NES = case_when(
      tblPatient..nesES == "No" ~ "NES",
      tblPatient..nesES == "Unknown" ~ "Unknown",
      tblPatient..nesES == "Yes" ~ "ES",
      TRUE ~ NA_character_
    ),
    NES = factor(NES, levels = c("NES", "Unknown", "ES"))
  )

# Now let's check
table(data2$NES)

# Now let's relevel this to have NES be the reference level
data2$NES <- relevel(data2$NES, ref = "ES")
```

# Power Analysis

Before we run our analysis, let's check to make sure that we have the appropriate amount of statistical power to actually make claims with our results.

For this analysis, we'll be using a two-sided significance level of **0.05** and a desired statistical power of **80%**. Additionally, given the novelty of our study, there isn't pilot data to infer our effect size from. Given that we are primarily using categorical or binary variables, we will assume a small to moderate effect size corresponding to an odds ratio of 1.3.

Our current dataset consists of an N of 617 patients...

```{r}

# Suppose you have 10 predictors
pwr.f2.test(u = 13, f2 = 0.05, sig.level = 0.05, power = 0.80)

# With 10 predictors in the saturaed model: we would need 334 subjects to be able to be confident in our results. Our sample size is sufficient.

```

# Aim 1:

## Overview:

The primary goal of Aim 1 is to fit a logistic regression model to better understand which covariates contribute to any specific patient's risk for non-adherence to the group therapy modality.

First, let's make out outcome a factor

```{r}
# Let's do one check of our class distribution again:
summary(data2$group_complete_50)
prop.table(table(data2$group_complete_50))
# Good, 258 High risk, and 359 low risk. 41.8% to 58.2%. Not too different.No need to rebalance the classes.

#set Group_Complete_50 as a factor:
data2$group_complete_50 <- factor(data2$group_complete_50)

# And set the names to be better suited to the model:
levels(data2$group_complete_50) <- c("HighRisk", "LowRisk") 

# And now let's set our reference to Low-Risk, as we're only interested in identifying patients that are high risk.
data2$group_complete_50 <- relevel(data2$group_complete_50, ref = "LowRisk")
```

## Assumptions:

Before we begin, I want to make sure that my assumptions have been met for Logisitic Regression.

### Linearity of the Log-Link:

The assumption is only needed for continuous variables. In our case, the only variables of interest would be age, Trauma events, and baseline seizure.

To test this, we can check to see if there is significant relationship between the variable of interest and it's log transformation. This is called the Box-Tidwell Test

```{r}

# We already have the log_age variable. So let's create the interaction term
data2$age_boxtid <- data2$age_log * data2$tblPatient..c_CurrentAge

# And now let's create the same variable for trauma events and baseline seizure.

data2$baseline_log <- log(data2$baseline_seizure)
data2$baseline_boxtid <- data2$baseline_log * data2$baseline_seizure

# And finally for trauma events

data2$trauma_log <- log(data2$trauma_events)
data2$trauma_boxtid <- data2$trauma_log * data2$trauma_events

# Also for Sx_Dx_time
data2$Sx_log <- log(data2$Sx_Dx_time)
data2$Sx_boxtid <- data2$Sx_log * data2$Sx_Dx_time
```

Great, now let's fit this to a logistic regression model:

```{r}

# Fit logistic regression model with interaction terms
model_bt <- glm(group_complete_50 ~ tblPatient..c_CurrentAge + baseline_seizure + Sx_Dx_time + trauma_events + age_boxtid + baseline_boxtid + Sx_boxtid + trauma_boxtid,
                data = data2, family = binomial)

# Examine significance of interaction terms
summary(model_bt)
```

None of the interaction terms are significant. Therefore, we can assume that all continuous variable meet the assumption of linearity.

## Saturated Model:

To begin, I want to start with a saturated model. This model will be the exact opposite of parsimonious, but I'd like to see if everything works. From there I will use Backwards selection with both AIC and BIC to determine the model of best fit.

Ok, now let's load all variables into a saturated model.

```{r}

# Ok, let's create a variable with all of the variable names

model_vars <- c("group_complete_50", "tblPatient..c_CurrentAge", "gender", "insurance", "disability", 
                "Sx_Dx_time", "partner", "employment", "anxiety_any", "depression_any", 
                "baseline_seizure", "trauma_events", "NES", "race")

# Let's drop the missing values
data2_aim1 <- na.omit(data2[model_vars])


# and run the model
model_saturated <- glm(group_complete_50 ~ tblPatient..c_CurrentAge + gender + insurance + disability + 
                             Sx_Dx_time + partner + employment + anxiety_any + depression_any + 
                             baseline_seizure + trauma_events + NES + race, 
                             data = data2_aim1, family = binomial)
# Get results
summary(model_saturated)


```

Ok, there are a few more variables that have popped here. Not too surprising:

-   As people age, they tend to be lower risk

-   If people have private insurance, they are lower risk

-   If they are unemployed they are higher risk

-   And if they are inconclusive on their assessment for ES, they are lower risk.

-   And if the subjects are non-white, they are at higher risk.

### VIF

Now let's check the VIF for multicollinearity:

```{r}

vif(model_saturated)
```

No VIF over 1.37: No multicollinearity

## Reduced Model

But there are far too many datapoint in this section. Let's use some backwards selection to determine which variables should be used.

```{r}

# Let's start with AIC
model_aic <- step(model_saturated, direction = "backward")
summary(model_aic)

# Add ORS and CI:
# Exponentiate coefficients (odds ratios)
OR <- exp(coef(model_aic))

# Exponentiate 95% confidence intervals
CI <- exp(confint(model_aic))  # This uses profile likelihood CIs

# Combine into a table
results <- cbind(OR, CI)
colnames(results) <- c("OR", "2.5 %", "97.5 %")

round(results, 2)
```

And now BIC

```{r}

n <- nrow(data2_aim1)  # use your model's dataset
model_bic <- step(model_saturated, direction = "backward", k = log(n))
summary(model_bic)

```

BIC doesn't want any variable other than the intercept. Therefore, we'll move forward with AIC. A model with AIC criterion applied chooses age, Employment status, NES diagnosis, and race.

Given the reduced model: the profile of someone who is high-risk is:

-   A young, non-white, unemployed individual with some confusion about their diagnosis.

### VIF

Now let's check for multicollinearity in the final model

```{r}

vif(model_aic)
```

# Aim 2:

## Dataframe creation

Let's try to train the logistic regression model first.

Before running, let's look for NAs. This will cause problems for us in the model training section.

```{r}

# Let's create a model training dataset to see if we can run the model first.

data2_model <- na.omit(data2[, c("group_complete_50", "tblPatient..c_CurrentAge", "insurance", "gender", "race", "employment", "baseline_seizure", "anxiety_any", "depression_any", "disability", "partner", "Sx_Dx_time", "tblPatient..nesES","trauma_events")])

```

## Model training

```{r}
# Define the control for cross-validation
train_control <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary, savePredictions = "final")

# Assign higher weight to HighRisk observations
weights <- ifelse(data2_model$group_complete_50 == "HighRisk", 2, 1)


# Train the logistic regression model
model_simple1 <- train(
  group_complete_50 ~ tblPatient..c_CurrentAge + insurance + gender + race + employment + baseline_seizure + anxiety_any + depression_any + disability + partner + Sx_Dx_time + trauma_events + tblPatient..nesES, 
  data = data2_model,
  method = "glm",
  family = "binomial",
  weights = weights,
  trControl = train_control
)

# View results
print(model_simple1)
summary(model_simple1$finalModel)

#the data isn't significant, but let's exponentiate the coefficients to get ORs
exp(coef(model_simple1$finalModel))
```

The total accuracy may be a bit low, but the sensitivity is quite high. This actually may be quite helpful for the sake of the clinic. In this case, High-Risk patients are considered the positive cases. This model was able to correctly identify High-Risk patients approximately \~80% of the time. However, it was only able to correctly indentify low-risk patients 23% of the time.

In an epidemiological sense, this effectively states that we have a quite low rate of false negatives, but quite a lot of false positives. For the purpose of Aim 2, this is actually quite good. The model may inflate the number of patients by putting truly low-risk patients in the high-risk group, but very rarely misses people who truly are High-risk. For the purpose of providing additional resources, this is highly beneficial. Patients will not be turned away due to this model.

```{r}

# Extract predicted probabilities and observed outcomes
pred <- model_simple1$pred

# Check the levels to ensure correct positive class
levels(pred$obs)  # positive class = second level

# Create ROC curve object
roc_obj <- roc(response = pred$obs, predictor = pred$HighRisk)

# Plot ROC
plot(roc_obj, col = "blue", main = "ROC Curve")

```

## Recursive Feature Elimination:

Now that we have a model that is *AT LEAST* sensitive, let's try to make this model more accurate overall. To do this, I plan to retrain the model with an iteratively fewer number of covariates. This way, we aim to reduce the overfitting and signal to noise ratio.

```{r}

# Define predictor names
features <- c("tblPatient..c_CurrentAge", "insurance", "gender", "race", "employment", "baseline_seizure", "anxiety_any", "depression_any", "disability", "partner", "Sx_Dx_time", "tblPatient..nesES","trauma_events")

# Define control parameters for RFE
rfe_control <- rfeControl(functions = rfFuncs, # Using Random Forest for feature importance
                         method = "cv",        # Cross-validation
                         number = 10)           # Number of folds

set.seed(123)
# Run RFE
rfe_results <- rfe(x = data2_model[, features],  # Predictor variables
                  y = data2_model[, "group_complete_50"],    # Outcome variable
                  sizes = c(1:13),       # Number of features to try
                  rfeControl = rfe_control)

# Get the selected features
selected_features <- predictors(rfe_results)

# Print results
print(rfe_results)
print(selected_features)

```

Using 10 -fold

:When viewing the effects of the model, the top 5 most important variables are Sx_Dx_time, baseline seizure count, employment, insurance, and race.

However. the model has the highest accuracy with 12 variables (.6097)

Using 5 fold:

Selects: Sx_Dx_time, baseline_seizure, and race

Let's run this model one more time to see the sensitivity:

```{r}

# Train the logistic regression model
model_simple2 <- train(
  group_complete_50 ~ Sx_Dx_time + baseline_seizure + employment + race + insurance + disability + gender + anxiety_any + partner + depression_any + tblPatient..c_CurrentAge + trauma_events, 
  data = data2_model,
  method = "glm",
  family = "binomial",
  #weights = weights,
  trControl = train_control
)

print(model_simple2)
```

ROC = .5499, Sensitivity = .81, Specificity .25

Let's visualize this:

```{r}

# Extract predicted probabilities and observed outcomes
pred <- model_simple2$pred

# Check the levels to ensure correct positive class
levels(pred$obs)  # positive class = second level

# Create ROC curve object
roc_obj <- roc(response = pred$obs, predictor = pred$HighRisk)

# Plot ROC
plot(roc_obj, col = "blue", main = "ROC Curve")
```

Now I want to look at Sensitivity vs Specificity to find possible thresholds:

```{r}

# Extract sensitivity and specificity across thresholds
roc_df <- coords(
  roc_obj,
  x = seq(0, 1, by = 0.01),
  input = "threshold",
  ret = c("threshold", "sensitivity", "specificity"),
  transpose = FALSE
)


# Plot it
ggplot(roc_df, aes(x = threshold)) +
  geom_line(aes(y = sensitivity, color = "Specificity"), linewidth = 1.2) +
  geom_line(aes(y = specificity, color = "Sensitivity"), linewidth = 1.2) +
  geom_vline(xintercept = 0.45, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = c("Sensitivity" = "blue", "Specificity" = "red")) +
  labs(
    y = "Rate",
    x = "Threshold",
    title = "Sensitivity and Specificity vs. Threshold (Logistic Regression)"
  ) +
  theme_minimal()

```

This looks ok, but I really want to move the threshold for classification back. a bit. Because it our case, it's more important to get high risk people identified, than provide additional resources to patients who do actually need it. Looking at this chart, it looks like if we move the threshold to \~.41, we would reach a better result. Let's try:

```{r}
# Get predicted probability of being HighRisk
pred_probs <- predict(model_simple2, newdata = data2_model, type = "prob")[, "HighRisk"]

# Apply custom threshold
custom_class <- ifelse(pred_probs > 0.48, "HighRisk", "LowRisk")

# Make sure both are factors with the same levels
custom_class <- factor(custom_class, levels = c("LowRisk", "HighRisk"))
reference <- factor(data2_model$group_complete_50, levels = c("LowRisk", "HighRisk"))

# Evaluate
confusionMatrix(
  factor(custom_class, levels = c("LowRisk", "HighRisk")),
  factor(data2_model$group_complete_50, levels = c("LowRisk", "HighRisk")),
  positive = "LowRisk"
)
```

```{r}
roc_obj <- roc(response = data2_model$group_complete_50,
               predictor = pred_probs[,"HighRisk"],
               levels = c("LowRisk", "HighRisk"))
plot(roc_obj)

```

# Lasso

Now that we have our final variables, I want to compare the results with Lasso regression. Let's move onto seeing which are actually needed and which are redundant. Lasso will move certain covariates to 0 if others are more predictive.

```{r}


# Select the 10 RFE-selected variables
rfe_vars <- c("tblPatient..c_CurrentAge", "insurance", "gender", "race", "employment", "baseline_seizure", "anxiety_any", "depression_any", "disability", "partner", "Sx_Dx_time", "tblPatient..nesES","trauma_events") 

# 2. Subset the data â€” ensure all selected variables exist and have no missing
# Subset and remove rows with missing values
selected_data <- na.omit(data2_model[, c("group_complete_50", rfe_vars)])

# Convert factor/character to numeric 0/1
selected_data$group_complete_50 <- ifelse(selected_data$group_complete_50 == "LowRisk", 1, 0)


# 3. Remove missing rows (if any)
selected_data <- na.omit(selected_data)


# 4. Create model matrix (excluding intercept)
x <- model.matrix(group_complete_50 ~ ., data = selected_data)[, -1]
y <- selected_data$group_complete_50



```

Now we can run cross validated Lasso:

```{r}

set.seed(123)

cv_lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial", type.measure = "auc")  # for auc
# or use type.measure = "class" for Accuracy optimization

# Predict probabilities
pred_probs <- predict(cv_lasso, newx = x, s = "lambda.min", type = "response")

# Predict classes
pred_class <- ifelse(pred_probs > 0.5, 1, 0)

# Accuracy
mean(pred_class == y)

# AUC
roc_obj <- roc(y, as.numeric(pred_probs))  # Ensure pred_probs is numeric
auc(roc_obj)


# Sensitivity and specificity at threshold 0.5
coords(roc_obj, x = 0.5, input = "threshold", ret = c("sensitivity", "specificity"))

# Plot ROC
plot(
  roc_obj,
  col = "blue",
  main = "ROC Curve",
  xaxs = "i",
  yaxs = "i"
)
```

Now let's look at sensitivty vs specificity

Pretty solid, but now i want to run this model using Random Forest first.

# Random Forest:

I ran this model first with logistic regression then with lasso, but this was hitting an accuracy limit of approximately 60%. This however, did have a high sensitivity, but was definitely lacking in terms of overall prediction. I'd like to see if Random Forest can provide a higher predictive value if we sacrifice interpretability. We'll start with rerunning RFE for Random Forest.

```{r}

# First we'll set our variable names to be used.
predictors <- c("tblPatient..c_CurrentAge", "insurance", "gender", "race", "employment", "baseline_seizure", "anxiety_any", "depression_any", "disability", "partner", "Sx_Dx_time", "tblPatient..nesES","trauma_events")


set.seed(123)



ctrl <- rfeControl(functions = rfFuncs,
                   method = "cv",
                   number = 10)

rfe_rf <- rfe(x = data2_model[, predictors],   # predictors is vector of var names
              y = data2_model$group_complete_50,
              sizes = c(1,2,3,4,5,6,7,8,9,10,11,12,13),   # try subsets of variables
              rfeControl = ctrl)


# Shows best variable set for random forest
rfe_rf$optVariables 
```

RFE tailored for Random Forest suggests only 8 variables: Baseline_seizure, Sx_Dx_time, employment, gender, disability, insurance, Ethnicity, race.

Great, now using these 8 variables, let's run the random forest model.

```{r}

# Set the selected variables in their own list
selected_vars <- c("baseline_seizure", "Sx_Dx_time", "employment", "gender", "insurance", "disability", "race", "depression_any", "anxiety_any", "partner", "tblPatient..c_CurrentAge", "trauma_events")

# create a new dataset with the cleaned values
rf_data <- data2_model[, c("group_complete_50", selected_vars)]

# Remove rows with missing values if they are present
rf_data <- na.omit(rf_data)

# And set LowRisk as the reference:
rf_data$group_complete_50 <- relevel(rf_data$group_complete_50, ref = "LowRisk")

# And finally, check the distribution of values in the factor:
table(rf_data$group_complete_50)
```

Now run the model:

```{r}

# Use 10-fold CV and evaluate on AUC
ctrl <- trainControl(method = "cv",
                     number = 10,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     savePredictions = "final")

set.seed(123)


rf_model <- train(group_complete_50 ~ .,
                  data = rf_data,
                  method = "rf",
                  trControl = ctrl,
                  metric = "Accuracy",      # Optimize for AUC
                  tuneLength = 13,
                  ntree = 1000)      # Auto-tunes mtry

print(rf_model)
```

Excellent, the random forest showed try 5 had a ROC of .63 with a sensitivty of .70 and specificty of .48. However, this was done at a threshold of .5. This may be too high of a threshold. Let's visualize this and move forward.

## Plotting:

First, let's plot the best threshold when comparing sensitivity and specificity.

```{r}

# First 
roc_obj <- roc(rf_data$group_complete_50, pred_probs)

plot(roc_obj, print.thres = "best", print.thres.best.method = "closest.topleft")
```

Ok, but for our purposes, sensitivity to High-risk patients is more important. Let's look at some thresholds for this model instead.

```{r}

# Your true outcomes and predicted probabilities
y_true <- rf_data$group_complete_50
#pred_probs <- predict(rf_model, newdata = rf_data, type = "prob")[, "HighRisk"]

# Create ROC object
roc_obj <- roc(y_true, pred_probs)

# Extract thresholds, sensitivities, and specificities
thresholds <- roc_obj$thresholds
sensitivities <- roc_obj$sensitivities
specificities <- roc_obj$specificities

# Plot sensitivity and specificity vs thresholds
plot(thresholds, sensitivities, type = "l", col = "blue", ylim = c(0,1),
     xlab = "Threshold", ylab = "Rate", main = "Sensitivity and Specificity vs Threshold")
lines(thresholds, specificities, col = "red")
legend("right", legend = c("Sensitivity", "Specificity"), col = c("blue", "red"), lty = 1)

# Choose the threshold. .4 seems appropriate.
chosen_threshold <- 0.5

# Add vertical line
abline(v = chosen_threshold, col = "darkgreen", lwd = 2, lty = 2)

# Find nearest index to chosen_threshold
idx <- which.min(abs(thresholds - chosen_threshold))

# Add text labels for sensitivity and specificity at that threshold
text(x = chosen_threshold, y = sensitivities[idx] + 0.05, 
     labels = paste0("Sens: ", round(sensitivities[idx], 2)), col = "blue", pos = 4)
text(x = chosen_threshold, y = specificities[idx] - 0.05, 
     labels = paste0("Spec: ", round(specificities[idx], 2)), col = "red", pos = 4)
```

Ok, great! When looking at a threshold of 0.3, We receive a sensitivity of .85 and a specificity of 0.61. This values went up as our threshold for determining who was at-risk changed. Looking at this graph, if we moved the dotted line to .5, our sensitivity goes up, but our specificity tanks. By being more liberal with who is considered High-Risk, we can increase the odds of allowing certain patients receive additional resources, while simultaneously factoring out the patients that are truly low risk. By lowering the bar, we choose to include edge cases, while having a good shot of making sure that the lowest risk patients aren't entered in, saving resources.

# Test:

```{r}

preds <- rf_model$pred %>%
  filter(mtry == rf_model$bestTune$mtry)  # only keep best tuning param

head(preds)


```

Step 3: Use `pROC` to calculate and plot Sensitivity & Specificity across thresholds

```{r}

roc_obj <- roc(preds$obs, preds$HighRisk, levels = rev(levels(preds$obs)))

thresholds <- seq(0, 1, by = 0.01)

roc_df <- coords(
  roc_obj,
  x = thresholds,
  input = "threshold",
  ret = c("threshold", "sensitivity", "specificity"),
  transpose = FALSE  # This keeps each row = one threshold
)
```

Plot Sensitivity & Specificity vs Threshold

```{r}

# Define your threshold of interest
chosen_threshold <- 0.3

# Find the row closest to the chosen threshold
annot_row <- roc_df[which.min(abs(roc_df$threshold - chosen_threshold)), ]

ggplot(roc_df, aes(x = threshold)) +
  geom_line(aes(y = sensitivity, color = "Sensitivity"), linewidth = 1.2) +
  geom_line(aes(y = specificity, color = "Specificity"), linewidth = 1.2) +
  
  # Vertical dashed line at chosen threshold
  geom_vline(xintercept = chosen_threshold, linetype = "dashed", color = "gray40") +
  
  # Annotate Sensitivity at chosen threshold
  geom_point(data = annot_row, aes(x = threshold, y = sensitivity), color = "blue", size = 3) +
  geom_text(
    data = annot_row,
    aes(
      x = threshold,
      y = sensitivity,
      label = paste0("Sens = ", round(sensitivity, 2))
    ),
    hjust = -0.1, vjust = -1, size = 3.5, color = "blue"
  ) +
  
  # Annotate Specificity at chosen threshold
  geom_point(data = annot_row, aes(x = threshold, y = specificity), color = "red", size = 3) +
  geom_text(
    data = annot_row,
    aes(
      x = threshold,
      y = specificity,
      label = paste0("Spec = ", round(specificity, 2))
    ),
    hjust = -0.1, vjust = 1.5, size = 3.5, color = "red"
  ) +
  
  scale_color_manual(values = c("Sensitivity" = "blue", "Specificity" = "red")) +
  labs(
    y = "Rate",
    x = "Threshold",
    title = "Sensitivity and Specificity vs. Threshold"
  ) +
  theme_minimal()

```

# XGBoost:

I want to check if a method utilizing gradient boosting would be helpful for model accuracy. Let's run this again and see what we can get:

First let's make the dataset

```{r}
# Set the selected variables in their own list
selected_vars <- c("tblPatient..c_CurrentAge", "insurance", "gender", "race", "employment", "baseline_seizure", "anxiety_any", "depression_any", "disability", "partner", "Sx_Dx_time", "tblPatient..nesES","trauma_events")

# create a new dataset with the cleaned values
xg_data <- data2_model[, c("group_complete_50", selected_vars)]

# Remove rows with missing values if they are present
xg_data <- na.omit(xg_data)

# And set LowRisk as the reference:
xg_data$group_complete_50 <- relevel(xg_data$group_complete_50, ref = "LowRisk")

# And finally, check the distribution of values in the factor:
table(xg_data$group_complete_50)
```

```{r}

ctrl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = "final"
)

# Train GBM model (gradient boosting machine)
set.seed(123)
gbm_model <- train(
  group_complete_50 ~ ., 
  data = xg_data,
  method = "gbm",
  metric = "ROC",
  trControl = ctrl,
  verbose = FALSE,
  tuneLength = 5
)

print(gbm_model)
```
